service: kinesis-auto-scaling
frameworkVersion: ">=2.64.1"
variablesResolutionMode: 20210326

provider:
  name: aws
  region: us-west-2
  deploymentBucket: ${file(env.yml):${self:custom.stage}.deployment-bucket}
  lambdaHashingVersion: "20201221"

custom:
  stage: "${opt:stage, self:provider.stage}"
  DRY_RUN_ENABLED:
    dev: true
    qa: true
    staging: true
    prod: false
  DRY_RUN: ${self:custom.DRY_RUN_ENABLED.${opt:stage, self:provider.stage}}
  SCALE_UP_THRESHOLD: 0.75
  SCALE_UP_MAX_ITER_AGE_MINS: 30
  SCALE_UP_EVALUATION_PERIOD: 5
  SCALE_UP_DATAPOINTS_REQUIRED: 5
  SCALE_DOWN_THRESHOLD: 0.25
  SCALE_DOWN_MIN_ITER_AGE_MINS: 1
  SCALE_DOWN_EVALUATION_PERIOD: 60
  SCALE_DOWN_DATAPOINTS_REQUIRED: 57

package:
  artifact: kinesis_scaling.zip

functions:
  #
  # Auto Scaling Lambda
  #
  KinesisAutoScaling:
    name: ${self:custom.stage}-${self:service}
    role: !GetAtt AutoScalingLambdaRole.Arn
    runtime: go1.x
    handler: main
    memorySize: 512
    timeout: 900
    maximumRetryAttempts: 0
    environment:
      THROTTLE_RETRY_MIN_SLEEP: 1
      THROTTLE_RETRY_MAX_SLEEP: 3
      THROTTLE_RETRY_COUNT: 30
      SCALE_PERIOD_MINS: 5 # kinesis_period_mins: cloudwatch datapoint period in minutes (default is 5, changing this is not recommended)
      SCALE_UP_THRESHOLD: ${self:custom.SCALE_UP_THRESHOLD}
      SCALE_UP_EVALUATION_PERIOD: ${self:custom.SCALE_UP_EVALUATION_PERIOD} # 25 Minutes / kinesis_period_mins. Evaluate scale up for 25 minutes.
      SCALE_UP_DATAPOINTS_REQUIRED: ${self:custom.SCALE_UP_DATAPOINTS_REQUIRED} # 25 Minutes / kinesis_period_mins. 5 out of 5 datapoints required. Takes 25 minutes above threshold to scale up.
      SCALE_UP_MAX_ITER_AGE_MINS: ${self:custom.SCALE_UP_MAX_ITER_AGE_MINS} # Scale up if max iterator age > x minutes
      SCALE_DOWN_THRESHOLD: ${self:custom.SCALE_DOWN_THRESHOLD}
      SCALE_DOWN_EVALUATION_PERIOD: ${self:custom.SCALE_DOWN_EVALUATION_PERIOD} # 300 Minutes / kinesis_period_mins. Evaluate scale down for 300 minutes.
      SCALE_DOWN_DATAPOINTS_REQUIRED: ${self:custom.SCALE_DOWN_DATAPOINTS_REQUIRED} # 285 Minutes / kinesis_period_mins. 57 out of 60 datapoints required. Takes 285 minutes below threshold to scale down.
      SCALE_DOWN_MIN_ITER_AGE_MINS: ${self:custom.SCALE_DOWN_MIN_ITER_AGE_MINS}
      PROCESSING_LAMBDA_ARN: "" # Empty = No Op, or if scaling a single stream, set this to the lambda consumer arn to automatically reserve concurrency as the stream scales
      PROCESSING_LAMBDAS_PER_SHARD: 5 # Lambdas per shard used by the above processing lambda
      DRY_RUN: ${self:custom.DRY_RUN} # Dry run flag (true or false) indicating whether streams will be scaling or not.

resources:
  Resources:
    KinesisAutoScalingLogGroup:
      Type: AWS::Logs::LogGroup
      Properties:
        RetentionInDays: 30

    AutoScalingLambdaAsyncConfig:
      Type: AWS::Lambda::EventInvokeConfig
      DependsOn: KinesisAutoScalingLambdaFunction
      Properties:
        FunctionName: ${self:custom.stage}-${self:service}
        MaximumRetryAttempts: 0  # We do not want any retries of the scaling function if it errors out, alarms will re-trigger it
        Qualifier: $LATEST

    #
    # Auto Scaling Lambda IAM Role
    #
    AutoScalingLambdaRole:
      Type: 'AWS::IAM::Role'
      Properties:
        AssumeRolePolicyDocument:
          Version: "2012-10-17"
          Statement:
            - Effect: Allow
              Principal:
                Service:
                  - lambda.amazonaws.com
              Action:
                - 'sts:AssumeRole'
        Path: /
        Policies:
          - PolicyName: AllowCreateCloudWatchAlarms
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: Allow
                  Action:
                    - 'cloudwatch:DescribeAlarms'
                    - 'cloudwatch:PutMetricAlarm'
                    - 'cloudwatch:ListTagsForResource'
                    - 'cloudwatch:SetAlarmState'
                    - 'cloudwatch:TagResource'
                  Resource:
                    - !Sub 'arn:aws:cloudwatch:${self:provider.region}:${AWS::AccountId}:alarm:${self:custom.stage}-*'
          - PolicyName: AllowCloudWatchMetrics
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: Allow
                  Action:
                    - 'cloudwatch:GetMetricData'
                    - 'cloudwatch:ListMetrics'
                    - 'cloudwatch:PutMetricData'
                  Resource:
                    - '*'
          - PolicyName: AllowLoggingToCloudWatch
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: Allow
                  Action:
                    - 'logs:CreateLogGroup'
                    - 'logs:CreateLogStream'
                    - 'logs:PutLogEvents'
                  Resource:
                    - !Sub 'arn:aws:logs:${self:provider.region}:${AWS::AccountId}:log-group:/aws/lambda/${self:custom.stage}-${self:service}:*'
          - PolicyName: AllowReadFromKinesis
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: Allow
                  Action:
                    - 'kinesis:DescribeStreamSummary'
                    - 'kinesis:AddTagsToStream'
                    - 'kinesis:ListTagsForStream'
                    - 'kinesis:UpdateShardCount'
                  Resource:
                    - !Sub 'arn:aws:kinesis:${self:provider.region}:${AWS::AccountId}:stream/${self:custom.stage}-*'
          - PolicyName: AllowPublishToSNS
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: Allow
                  Action:
                    - 'sns:Publish'
                  Resource:
                    - !Sub 'arn:aws:sns:${self:provider.region}:${AWS::AccountId}:${AWS::StackName}-kinesis-scaling-topic'

    #
    # Auto Scaling SNS Topic
    #
    AutoScalingSNSTopic:
      Type: AWS::SNS::Topic
      DependsOn: KinesisAutoScalingLambdaFunction
      Properties:
        Subscription:
          - Endpoint: !Sub 'arn:aws:lambda:${self:provider.region}:${AWS::AccountId}:function:${self:custom.stage}-${self:service}'
            Protocol: "lambda"
        TopicName: !Sub '${AWS::StackName}-kinesis-scaling-topic'

    AutoScalingSNSTopicLambdaPerm:
      Type: AWS::Lambda::Permission
      Properties:
        Action: 'lambda:InvokeFunction'
        FunctionName: ${self:custom.stage}-${self:service}
        Principal: "sns.amazonaws.com"
        SourceArn: !Ref AutoScalingSNSTopic

    #
    # Kinesis discount-code-requests - Scale Up Alarm
    #
    AutoScalingDiscountCodeRequestsScaleUpAlarm:
      Type: AWS::CloudWatch::Alarm
      DependsOn: AutoScalingSNSTopic
      Properties:
        AlarmName: "${self:custom.stage}-discount-code-requests-scale-up"
        AlarmDescription: 'Stream throughput has gone above the scale up threshold'
        ComparisonOperator: GreaterThanOrEqualToThreshold
        Threshold: ${self:custom.SCALE_UP_THRESHOLD}
        EvaluationPeriods: ${self:custom.SCALE_UP_EVALUATION_PERIOD} # 25 Minutes / kinesis_period_mins. Evaluate scale up for 25 minutes.
        DatapointsToAlarm: ${self:custom.SCALE_UP_DATAPOINTS_REQUIRED} # 25 Minutes / kinesis_period_mins. 5 out of 5 datapoints required. Takes 25 minutes above threshold to scale up.
        AlarmActions: [ !Ref AutoScalingSNSTopic ]
        Metrics:
          - Id: s1
            ReturnData: False
            Label: ShardCount
            Expression: 1
          - Id: s2
            ReturnData: False
            Label: MaxIteratorAgeToScaleUp
            Expression: ${self:custom.SCALE_UP_MAX_ITER_AGE_MINS}
          - Id: m1
            ReturnData: False
            Label: IncomingBytes
            MetricStat:
              Stat: Sum
              Period: 300 # Must match kinesis_period_mins: 5 minutes. Convert to seconds. 300 seconds.
              Metric:
                MetricName: IncomingBytes
                Namespace: AWS/Kinesis
                Dimensions:
                  - Name: StreamName
                    Value: ${self:custom.stage}-discount-code-requests
          - Id: m2
            ReturnData: False
            Label: IncomingRecords
            MetricStat:
              Stat: Sum
              Period: 300 # Must match kinesis_period_mins: 5 minutes. Convert to seconds. 300 seconds.
              Metric:
                MetricName: IncomingRecords
                Namespace: AWS/Kinesis
                Dimensions:
                  - Name: StreamName
                    Value: ${self:custom.stage}-discount-code-requests
          - Id: m3
            ReturnData: False
            Label: GetRecords.IteratorAgeMilliseconds
            MetricStat:
              Stat: Maximum
              Period: 300 # Must match kinesis_period_mins: 5 minutes. Convert to seconds. 300 seconds.
              Metric:
                MetricName: GetRecords.IteratorAgeMilliseconds
                Namespace: AWS/Kinesis
                Dimensions:
                  - Name: StreamName
                    Value: ${self:custom.stage}-discount-code-requests
          - Id: e1
            ReturnData: False
            Label: FillMissingDataPointsWithZeroForIncomingBytes
            Expression: FILL(m1,0)
          - Id: e2
            ReturnData: False
            Label: FillMissingDataPointsWithZeroForIncomingRecords
            Expression: FILL(m2,0)
          - Id: e3
            ReturnData: False
            Label: IncomingBytesUsageFactor
            Expression: e1/(1024*1024*60*5*s1) # e1/(1024*1024*60*${kinesis_period_mins}*s1)
          - Id: e4
            ReturnData: False
            Label: IncomingRecordsUsageFactor
            Expression: e2/(1000*60*5*s1) # e2/(1000*60*${kinesis_period_mins}*s1)
          - Id: e5
            ReturnData: False
            Label: IteratorAgeAdjustedFactor
            Expression: !Sub (FILL(m3,0)/1000/60)*(${self:custom.SCALE_UP_THRESHOLD}/s2) # We want to scale up when IterAge is > 30 mins
          - Id: e6
            ReturnData: True
            Label: MaxIncomingUsageFactor
            Expression: MAX([e3,e4,e5]) # Take the highest usage factor between bytes/sec, records/sec, and adjusted iterator age

    AutoScalingDiscountCodeRequestsScaleDownAlarm:
      Type: AWS::CloudWatch::Alarm
      DependsOn: AutoScalingSNSTopic
      Properties:
        AlarmName: "${self:custom.stage}-discount-code-requests-scale-down"
        AlarmDescription: 'Stream throughput has gone below the scale down threshold'
        ComparisonOperator: LessThanThreshold
        Threshold: -1 # Set to -1 for 1 shard stream, as we can't scale down from here. Gets updated by the auto scaling lambda later
        EvaluationPeriods: ${self:custom.SCALE_DOWN_EVALUATION_PERIOD} # 300 Minutes / kinesis_period_mins. Evaluate scale down for 300 minutes.
        DatapointsToAlarm: ${self:custom.SCALE_DOWN_DATAPOINTS_REQUIRED} # 285 Minutes / kinesis_period_mins. 57 out of 60 datapoints required. Takes 285 minutes below threshold to scale down.
        AlarmActions: [ !Ref AutoScalingSNSTopic ]
        Metrics:
          - Id: s1
            ReturnData: False
            Label: ShardCount
            Expression: 1
          - Id: s2
            ReturnData: False
            Label: IteratorAgeMinutesToBlockScaledowns
            Expression: ${self:custom.SCALE_DOWN_MIN_ITER_AGE_MINS}
          - Id: m1
            ReturnData: False
            Label: IncomingBytes
            MetricStat:
              Stat: Sum
              Period: 300 # Must match kinesis_period_mins: 5 minutes. Convert to seconds. 300 seconds.
              Metric:
                MetricName: IncomingBytes
                Namespace: AWS/Kinesis
                Dimensions:
                  - Name: StreamName
                    Value: "${self:custom.stage}-discount-code-requests"
          - Id: m2
            ReturnData: False
            Label: IncomingRecords
            MetricStat:
              Stat: Sum
              Period: 300 # Must match kinesis_period_mins: 5 minutes. Convert to seconds. 300 seconds.
              Metric:
                MetricName: IncomingRecords
                Namespace: AWS/Kinesis
                Dimensions:
                  - Name: StreamName
                    Value: ${self:custom.stage}-discount-code-requests
          - Id: m3
            ReturnData: False
            Label: GetRecords.IteratorAgeMilliseconds
            MetricStat:
              Stat: Maximum
              Period: 300 # Must match kinesis_period_mins: 5 minutes. Convert to seconds. 300 seconds.
              Metric:
                MetricName: GetRecords.IteratorAgeMilliseconds
                Namespace: AWS/Kinesis
                Dimensions:
                  - Name: StreamName
                    Value: ${self:custom.stage}-discount-code-requests
          - Id: e1
            ReturnData: False
            Label: FillMissingDataPointsWithZeroForIncomingBytes
            Expression: FILL(m1,0)
          - Id: e2
            ReturnData: False
            Label: FillMissingDataPointsWithZeroForIncomingRecords
            Expression: FILL(m2,0)
          - Id: e3
            ReturnData: False
            Label: IncomingBytesUsageFactor
            Expression: e1/(1024*1024*60*5*s1) # e1/(1024*1024*60*${kinesis_period_mins}*s1)
          - Id: e4
            ReturnData: False
            Label: IncomingRecordsUsageFactor
            Expression: e2/(1000*60*5*s1) # e2/(1000*60*${kinesis_period_mins}*s1)
          - Id: e5
            ReturnData: False
            Label: IteratorAgeAdjustedFactor
            Expression: !Sub (FILL(m3,0)/1000/60)*(${self:custom.SCALE_DOWN_THRESHOLD}/s2) # We want to block scaledowns when IterAge is > 30 mins, multiply IterAge so 30 mins = <alarmThreshold>
          - Id: e6
            ReturnData: True
            Label: MaxIncomingUsageFactor
            Expression: MAX([e3,e4,e5]) # Take the highest usage factor between bytes/sec, records/sec, and adjusted iterator age